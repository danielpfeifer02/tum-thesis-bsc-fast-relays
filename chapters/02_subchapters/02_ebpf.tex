\section{eBPF}\label{sec:ebpf_bg}
In 1992 a technology called \textit{Berkeley Packet Filter} (BPF) was introduced into 
the Unix kernel.
By using BPF, it is possible to attach a small BPF program to some pre-defined hook points in 
the network stack of the kernel and filter packets there in a stateless manner.
This provided more efficiency since the packets did not need to be copied into 
userspace anymore but could be processed directly in the kernel.
A need for better tracing capabilities of the Linux kernel then led to the development 
of an extended version of BPF called \textit{eBPF} which was introduced in 2014 and 
heavily influenced by a tracing tool called \textit{dtrace}, which allows for 
real-time inspection of running processes, memory- and CPU-usages, network-resources 
and more~\parencite{ebpf-intro-tigera}.

\subsection{eBPF Hook Points}
The Linux kernel offers several hook points to which eBPF programs can be attached.
There are two prominent ones that we considered for our suggested setup.
The first allows access to the \textit{Traffic Control} (TC) subsystem, while the 
second allows access to the \textit{eXpress Data Path} (XDP) subsystem.

XDP would generally provide a better performance since it is located 
lower on the network stack, namely directly in the NIC driver, than the 
TC-hook point, which is located in the link layer.
On the other hand, TC offers a more versatile way of packet processing since 
the used \verb|sk_buff| buffer object containing the packet data provides access to metadata, which is unavailable when using XDP and its \verb|xdp_buff| buffer object.
% https://liuhangbin.netlify.app/post/ebpf-and-xdp/ % TODO: is this citable?
What ultimately led us to choose TC over XDP was, however, the fact that 
XDP only allows ingress packet processing, while TC allows processing packets at ingress and egress.
That means that with XDP, we would not have been able to redirect packets to be handled 
at egress, which is crucial for the fast-relay setup we aim for. % TODO: cannot redirect from XDP to TC?

% The XDP hook, which is directly located in the NIC-driver, lies lower in the network 
% stack than the TC-hook, which is located in the link-layer.
% Despite being higher up in the network stack, the TC-hook has the big advantage that
% it offers ingress and egress processing while the XDP-hook is available for ingress 
% processing only.
% This makes the XDP-hook suboptimal for implementing fast-relays since 
% they heavily rely on processing packets at egress after those were redirected
% from ingress.
\autoref{fig:ebpf-hooks} illustrates again the relative positions of the TC and
XDP hook points in the network stack.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/02_background/hook-point-locations.drawio.pdf}
    \caption[Hook points within network stack]{TC and XDP hook points in the Linux kernel.
    The red loop indicates the shortcut, which the fast-relay utilizes.
    }\label{fig:ebpf-hooks}
\end{figure}

% \subsection{Traffic Control Queuing Disciplines} % TODO: needed?
% The Linux Traffic Control Subsystem uses Queuing Disciplines (qdiscs) to define how packets
% are handled. TODO

\subsection{eBPF Verifier}
Since eBPF programs are executed in the kernel, it is quite obvious that extensive
security checks need to be in place to ensure that the kernel does not experience 
problems like infinite loops, accesses to invalid memory locations, or other security 
related issues.
This explains the existence of the so-called \textit{eBPF verifier} which inspects 
every eBPF program for its safety by simulating possible program paths, 
looking at the graph representation of the program and more~\parencite{ebpf-verifier}.
This imposes some restrictions on the complexity of the programs that can be 
used within the kernel.
For our early prototype implementation, this does not cause any issues, since we do not need 
to rely on very complex control structures.
However, if one wants to extend the prototype to support a more complicated packet structure
(e.g.~composition of multiple frames per packet), the verifier might become a limiting factor.


\subsection{Important eBPF Concepts}
One of the most important concepts in eBPF, which we use quite extensively, is 
the \textit{eBPF map}.
Such a map boils down to a section in memory that is reserved for the eBPF program
and which can be used as a key-value store for arbitrary data.
This part of memory can then also be accessed from userspace and thus provides the main 
way of communication between the eBPF program and our application.


\vspace{0.5cm}
\noindent\begin{minipage}{\textwidth}
    \begin{lstlisting}[style=CStyle,caption={Examplary eBPF map definitions.}, label={lst:ebpf-map}]
        struct {
            __uint(type, BPF_MAP_TYPE_HASH);        // Hash map
            __type(key, struct client_info_key_t);  // Specific client key
            __type(value, uint32_t);                // 32 bit id
            __uint(max_entries, MAX_CLIENTS);       // Maximum number of clients
            __uint(pinning, LIBBPF_PIN_BY_NAME);    // Pin by name to the tc filesystem
        } client_id SEC(".maps");

        struct {
            __uint(type, BPF_MAP_TYPE_RINGBUF);     // Ring buffer
            __uint(max_entries, MAX_PACKET_EVENTS); // Maximum number of packet events
            __uint(pinning, LIBBPF_PIN_BY_NAME);    // Pin by name to the tc filesystem
        } packet_events SEC(".maps");
    \end{lstlisting}
\end{minipage}
\vspace{0.5cm}

When we define an eBPF map, we can choose between different types and configure
size, key type, value type, and how the map is stored. % TODO: surely there is more one can define
An example of two eBPF map definitions can be seen in~\autoref{lst:ebpf-map}.
It shows two different types of maps, a hash map, and a ring buffer, that are used in
our fast-relay setup.
Those and other relevant map types are listed in~\autoref{tab:ebpf-map-types}.


\vspace{0.5cm}

\begin{table}[H]
    \centering
    \begin{tabular}{L{7cm}L{7cm}}
        \toprule
            Type & Description \\
        \midrule
            BPF\_MAP\_TYPE\_HASH & A hash map where keys and values can be arbitrarily defined. \\
        \midrule
            BPF\_MAP\_TYPE\_PERCPU\_HASH & A hash map with separate value slots for each CPU, providing improved performance in multi-core environments. \\
        \midrule
            BPF\_MAP\_TYPE\_ARRAY & An array map that allows random access to elements by index. \\
        \midrule
            BPF\_MAP\_TYPE\_PERCPU\_ARRAY & An array map with separate value slots for each CPU, useful for per-CPU data storage. \\
        \midrule
            BPF\_MAP\_TYPE\_RINGBUF & A ring buffer for implementing high-performance data queues. \\
        \bottomrule
    \end{tabular}
    \caption[Subset of eBPF map types]{Some eBPF map types. (defined in /usr/include/linux/bpf.h)}\label{tab:ebpf-map-types}
\end{table}

