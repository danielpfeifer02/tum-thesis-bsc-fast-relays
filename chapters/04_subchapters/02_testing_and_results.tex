\section{Testing and Results}\label{sec:testing_and_results}
When testing the performance of our prototype we will focus on showing that the eBPF forwarding
is capable of reducing the delay of packets.
Delay in our case is measured again using eBPF programs that save the timestamps of a packet 
leaving and entering the server and client namespace respectively.
Due to the highly deterministic nature of the local environment we know that the difference 
between the two timestamps will be mainly due to processing at the relay.
Besides delay analysis we will also show that our approach does not require more CPU resources
than the plain userspace forwarding.
For that we will look at profiling data from programs like ``pprof'' % TODO: an extra obe for eBPF programs?
as well as the Linux ``process status'' (\verb|ps|) command which can be used to 
show the CPU usage of a process. % TODO: mention the "ax" parameter?

\subsection{Delay Reduction of eBPF Forwarding}
When considering the impact of eBPF-Forwarding on the delays of packets, figure~\ref{fig:delay-improvement}
visualizes the timestamp and delay data that was the result of a rudimentary test of a single-connection 
scenario that was run both with and without direct eBPF forwarding.
We can see that the delay of a single packet is decreased by around 100\,Âµs
when compared to the userspace forwarding. 
The userspace relay setup for this measurement only consideres the simplest case of a single client connection 
and a direct ``passing-through'' of the packets without much additional computation.
More complex setups might have the relay consider tasks like (de-)multiplexing, 
encoding changes, error correction or similar. 
Given that such complexity can become arbitrarily large, this delay improvement can become even bigger.
\\
Another thing figure~\ref{fig:delay-improvement} shows is that the delay has a smaller variance due to the fact 
that the eBPF program path is somewhat similar for each packet whereas, in contrast, the userspace path can have 
buffers, queues, or equivalent structures thatcan lead to a higher difference in processing time between packets. 
This effect however might be less observable in a real world scenario due to the general network jitter which 
might outweigh the reduction in jitter that our setup caused.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/04_testing_and_results/delays_small_packets_simple_userspace.pdf}
    \caption[Delay analysis of eBPF approach]{Userspace avoidance causes a reduction in delay and jitter.}\label{fig:delay-improvement}
\end{figure}

\subsection{CPU Utilization Comparison}
Besides showing that our approach reduces packet delay we also measured that the CPU usage is 
not negatively impacted by our streaming system.
Figures~\ref{fig:cpu-utilization-server},~\ref{fig:cpu-utilization-relay} and~\ref{fig:cpu-utilization-client}
show the CPU usage of the server, relay and client namespace, respectively, both with and without eBPF forwarding.
It is observable that none of the utilizations significantly differs between the two setups. 

\begin{figure}[H]
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/04_testing_and_results/cpu_usage_server_ns.pdf}
        \caption[Server CPU usage comparison]{Server namespace CPU usage comparison.}\label{fig:cpu-utilization-server}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/04_testing_and_results/cpu_usage_relay_ns.pdf}
        \caption[Relay CPU usage comparison]{Relay namespace CPU usage comparison.}\label{fig:cpu-utilization-relay}
    \end{minipage}\hfill
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=0.48\textwidth]{figures/04_testing_and_results/cpu_usage_client_ns.pdf}
        \caption[Client CPU usage comparison]{Client namespace CPU usage comparison.}\label{fig:cpu-utilization-client}
    \end{minipage}
\end{figure}

We also looked more closely which parts of our system take up the most CPU time and compared them
to the most CPU-intensive parts of the initial userspace forwarding.
Table~\ref{tab:cpu-usage-top} shows the top 10 functions that take up the most CPU time in our newly
developed setup while table~\ref{tab:non-ebpf-cpu-usage-top} shows the same for the old userspace 
forwarding setup without any eBPF interference. % TODO: make sure reference labels are correct
We can clearly see that our setup is executing a lot of system calls which is to be expected
due to the ongoing communication between the userspace and the kernel (i.e.~eBPF maps).
TODO: measure non ebpf table and compare specific differences 

\input{chapters/04_subchapters/cpu_usage_tables/table_top.tex} % TODO: also include table for "normal" relay and compare difference in what functions use most CPU 
% \input{chapters/04_subchapters/cpu_usage_tables/table_top_cum.tex} % TODO: this is kinda useless since it only shows the "wrapping" functions. Maybe remove?
\input{chapters/04_subchapters/cpu_usage_tables/non_ebpf_table_top.tex} % TODO: actually measure